.libPaths("C:\\SOFTWARE\\R-4.1.2\\library")
library(raster)
library(rgdal)
library(landscapemetrics)
library(landscapetools)
library(pbapply) ## adds progress bar to apply
library(data.table) ## rbindlist - makes one data table of many
library(tidyverse) ## tibble functions
library(rgeos)
library(sp) ## coordinates
wd.env <- "E:/NON_PROJECT/DORMOUSE_NE/GIS/100m"
wd.out <- "E:/NON_PROJECT/DORMOUSE_NE/GIS/100m"
OSGB.proj <- '+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +datum=OSGB36 +units=m +no_defs'
### Get the 100m grid on which to calculate fragmentation. Created by original frag_500m code.
m100.dd <- raster(paste0(wd.env, "/OSGB_Grid_100m_clim_dd.tif"))
t100 <- raster("E:/GIS_DATA/LandCover/UK/Devon_THaW/thaw_region.tif")
m100.t <- m100.dd*t100
## Vegetation from THaW data
treehedge <- raster("E:/GIS_DATA/LandCover/UK/Devon_THaW/trees_hedge_1m.tif")
scrub <- raster("E:/GIS_DATA/LandCover/UK/Devon_THaW/scrub_1m.tif")
## Get 100m cell numbers inside the ThaW region
xy <- xyFromCell(m100.t, 1:ncell(m100.t))
xyz <- na.omit(cbind(xy, raster::extract(m100.t, xy)))
cells <- cellFromXY(m100.t, xyz[,c("x","y")])
rm(xy)
clumpy.fn <- function (cell) {
# Make a mask for the target 100m cell and the surrounding cells within a 500m radius
xy <- as.data.frame(xyFromCell(m100.t, cell)) ## coordinates of the focal 100mcell
coordinates(xy) <- ~x+y ## make spatial object
buf500 <- gBuffer(xy, width=500) ## calculate the buffer
# Extract the 1m cells that are within the buffer around the focal 100m cell
f1 <- crop(treehedge, extent(buf500))
f1 <- raster::mask(f1, buf500)
# Calculate clumpiness if the area of forest in the target 100m cell permits
# f1.clumpy <- c.blank
if(cellStats(f1, sum) == 0) { ## There is no trees/hedge in the 500m buffer
clumpy <- connect <- pacfrac <- -3
ta <- 0
}
if(cellStats(f1, sum) == 1) { ## Only one pixel in the 500m buffer is trees/hedge
clumpy <- connect <- pacfrac <- -2
ta <- 1
}
if(cellStats(f1, sum) == 10000) { ## All values in the 500m buffer are trees/hedge
clumpy <- connect <- pacfrac <- 2
ta <- raster::area(buf500) ## m2
}
if(!(cellStats(f1, sum) %in% c(0, 1, 10000))) { ## Clumpiness can be calculated
clumpy <- lsm_c_clumpy(f1)
clumpy <- clumpy[clumpy$class==1,]$value ## The clumpiness of the forest only
connect <- lsm_l_contag(f1)$value ## name is 'connectance' in list_lsm
pacfrac <- lsm_l_pafrac(f1)$value
# prox_cv <- ... enn?
m <- matrix(c(-1,0.5,NA), byrow=T, nrow=1)
test <- reclassify(f1, m)
ta <- lsm_l_ta(test)$value ## total area of focal habitat type
}
out <- cbind(cell, coordinates(xy), clumpy, connect, pacfrac, ta)
# f1.clumpy$prop <- cellStats(f1, sum)/10000 ## this is the proportion of teh entire buffer, and the buffer may include non-landscape areas
print(out)
return(out)
}
### Apply over all cells in 100m raster
fragstats <- pblapply(cells[1], clumpy.fn) ## 1:ncell(m100)
write.csv(fragstats, paste0(wd.out, "/fragstats_500m_treehedge_t.csv"), row.names = FALSE)
fileConn<-file(paste0(wd.out, "/fragstats_500m_treehedge_t.csv"))
writeLines(c("Hello","World"), fileConn)
close(fileConn)
wd.out
?writeLines
write.csv(fragstats, file=paste0(wd.out, "/fragstats_500m_treehedge_t.csv"), sep=",", append=TRUE, quote=FALSE,
col.names=FALSE, row.names=FALSE)
write.table(fragstats, file=paste0(wd.out, "/fragstats_500m_treehedge_t.csv"), sep=",", append=TRUE, quote=FALSE,
col.names=FALSE, row.names=FALSE)
write.csv(fragstats, paste0(wd.out, "/fragstats_500m_treehedge_t.csv"), row.names = FALSE)
write.table(fragstats, file=paste0(wd.out, "/fragstats_500m_treehedge_t.csv"), sep=",", append=TRUE, quote=FALSE,
col.names=FALSE, row.names=FALSE)
write.table(fragstats, file=paste0(wd.out, "/fragstats_500m_treehedge_t.csv"), sep=",", append=TRUE, quote=FALSE,
col.names=FALSE, row.names=FALSE)
write.table(fragstats, file=paste0(wd.out, "/fragstats_500m_treehedge_t.csv"), sep=",", append=TRUE, quote=FALSE,
col.names=FALSE, row.names=FALSE)
cell <- cells[1]
cell
clumpy.fn <- function (cell) {
# Make a mask for the target 100m cell and the surrounding cells within a 500m radius
xy <- as.data.frame(xyFromCell(m100.t, cell)) ## coordinates of the focal 100mcell
coordinates(xy) <- ~x+y ## make spatial object
buf500 <- gBuffer(xy, width=500) ## calculate the buffer
# Extract the 1m cells that are within the buffer around the focal 100m cell
f1 <- crop(treehedge, extent(buf500))
f1 <- raster::mask(f1, buf500)
# Calculate clumpiness if the area of forest in the target 100m cell permits
# f1.clumpy <- c.blank
if(cellStats(f1, sum) == 0) { ## There is no trees/hedge in the 500m buffer
clumpy <- connect <- pacfrac <- -3
ta <- 0
}
if(cellStats(f1, sum) == 1) { ## Only one pixel in the 500m buffer is trees/hedge
clumpy <- connect <- pacfrac <- -2
ta <- 1
}
if(cellStats(f1, sum) == 10000) { ## All values in the 500m buffer are trees/hedge
clumpy <- connect <- pacfrac <- 2
ta <- raster::area(buf500) ## m2
}
if(!(cellStats(f1, sum) %in% c(0, 1, 10000))) { ## Clumpiness can be calculated
clumpy <- lsm_c_clumpy(f1)
clumpy <- clumpy[clumpy$class==1,]$value ## The clumpiness of the forest only
connect <- lsm_l_contag(f1)$value ## name is 'connectance' in list_lsm
pacfrac <- lsm_l_pafrac(f1)$value
# prox_cv <- ... enn?
m <- matrix(c(-1,0.5,NA), byrow=T, nrow=1)
test <- reclassify(f1, m)
ta <- lsm_l_ta(test)$value ## total area of focal habitat type
}
out <- cbind(cell, coordinates(xy), clumpy, connect, pacfrac, ta)
# f1.clumpy$prop <- cellStats(f1, sum)/10000 ## this is the proportion of teh entire buffer, and the buffer may include non-landscape areas
print(out)
write.table(out, file=paste0(wd.out, "/fragstats_500m_treehedge_t.csv"), sep=",", append=TRUE, quote=FALSE,
col.names=FALSE, row.names=FALSE)
return(out)
}
clumpy.fn <- function (cell) {
# Make a mask for the target 100m cell and the surrounding cells within a 500m radius
xy <- as.data.frame(xyFromCell(m100.t, cell)) ## coordinates of the focal 100mcell
coordinates(xy) <- ~x+y ## make spatial object
buf500 <- gBuffer(xy, width=500) ## calculate the buffer
# Extract the 1m cells that are within the buffer around the focal 100m cell
f1 <- crop(treehedge, extent(buf500))
f1 <- raster::mask(f1, buf500)
# Calculate clumpiness if the area of forest in the target 100m cell permits
# f1.clumpy <- c.blank
if(cellStats(f1, sum) == 0) { ## There is no trees/hedge in the 500m buffer
clumpy <- connect <- pacfrac <- -3
ta <- 0
}
if(cellStats(f1, sum) == 1) { ## Only one pixel in the 500m buffer is trees/hedge
clumpy <- connect <- pacfrac <- -2
ta <- 1
}
if(cellStats(f1, sum) == 10000) { ## All values in the 500m buffer are trees/hedge
clumpy <- connect <- pacfrac <- 2
ta <- raster::area(buf500) ## m2
}
if(!(cellStats(f1, sum) %in% c(0, 1, 10000))) { ## Clumpiness can be calculated
clumpy <- lsm_c_clumpy(f1)
clumpy <- clumpy[clumpy$class==1,]$value ## The clumpiness of the forest only
connect <- lsm_l_contag(f1)$value ## name is 'connectance' in list_lsm
pacfrac <- lsm_l_pafrac(f1)$value
# prox_cv <- ... enn?
m <- matrix(c(-1,0.5,NA), byrow=T, nrow=1)
test <- reclassify(f1, m)
ta <- lsm_l_ta(test)$value ## total area of focal habitat type
}
out <- cbind(cell, coordinates(xy), clumpy, connect, pacfrac, ta)
# f1.clumpy$prop <- cellStats(f1, sum)/10000 ## this is the proportion of teh entire buffer, and the buffer may include non-landscape areas
print(out)
write.table(out, file=paste0(wd.out, "/fragstats_500m_treehedge_t.csv"), sep=",", append=TRUE, quote=FALSE,
col.names=FALSE, row.names=FALSE)
return(out)
}
cell <- cells[2]
cell
clumpy.fn <- function (cell) {
# Make a mask for the target 100m cell and the surrounding cells within a 500m radius
xy <- as.data.frame(xyFromCell(m100.t, cell)) ## coordinates of the focal 100mcell
coordinates(xy) <- ~x+y ## make spatial object
buf500 <- gBuffer(xy, width=500) ## calculate the buffer
# Extract the 1m cells that are within the buffer around the focal 100m cell
f1 <- crop(treehedge, extent(buf500))
f1 <- raster::mask(f1, buf500)
# Calculate clumpiness if the area of forest in the target 100m cell permits
# f1.clumpy <- c.blank
if(cellStats(f1, sum) == 0) { ## There is no trees/hedge in the 500m buffer
clumpy <- connect <- pacfrac <- -3
ta <- 0
}
if(cellStats(f1, sum) == 1) { ## Only one pixel in the 500m buffer is trees/hedge
clumpy <- connect <- pacfrac <- -2
ta <- 1
}
if(cellStats(f1, sum) == 10000) { ## All values in the 500m buffer are trees/hedge
clumpy <- connect <- pacfrac <- 2
ta <- raster::area(buf500) ## m2
}
if(!(cellStats(f1, sum) %in% c(0, 1, 10000))) { ## Clumpiness can be calculated
clumpy <- lsm_c_clumpy(f1)
clumpy <- clumpy[clumpy$class==1,]$value ## The clumpiness of the forest only
connect <- lsm_l_contag(f1)$value ## name is 'connectance' in list_lsm
pacfrac <- lsm_l_pafrac(f1)$value
# prox_cv <- ... enn?
m <- matrix(c(-1,0.5,NA), byrow=T, nrow=1)
test <- reclassify(f1, m)
ta <- lsm_l_ta(test)$value ## total area of focal habitat type
}
out <- cbind(cell, coordinates(xy), clumpy, connect, pacfrac, ta)
# f1.clumpy$prop <- cellStats(f1, sum)/10000 ## this is the proportion of teh entire buffer, and the buffer may include non-landscape areas
print(out)
write.table(out, file=paste0(wd.out, "/fragstats_500m_treehedge_t.csv"), sep=",", append=TRUE, quote=FALSE,
col.names=FALSE, row.names=FALSE)
return(out)
}
### Apply over all cells in 100m raster
fragstats <- pblapply(cells[2], clumpy.fn) ## 1:ncell(m100)
.libPaths("C:\\SOFTWARE\\R-4.1.2\\library")
library(raster)
library(rgdal)
library(landscapemetrics)
library(landscapetools)
library(pbapply) ## adds progress bar to apply
library(data.table) ## rbindlist - makes one data table of many
library(tidyverse) ## tibble functions
library(rgeos)
library(sp) ## coordinates
wd.env <- "E:/NON_PROJECT/DORMOUSE_NE/GIS/100m"
wd.out <- "E:/NON_PROJECT/DORMOUSE_NE/GIS/100m"
OSGB.proj <- '+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +ellps=airy +datum=OSGB36 +units=m +no_defs'
### Get the 100m grid on which to calculate fragmentation. Created by original frag_500m code.
m100.dd <- raster(paste0(wd.env, "/OSGB_Grid_100m_clim_dd.tif"))
t100 <- raster("E:/GIS_DATA/LandCover/UK/Devon_THaW/thaw_region.tif")
m100.t <- m100.dd*t100
## Vegetation from THaW data
treehedge <- raster("E:/GIS_DATA/LandCover/UK/Devon_THaW/trees_hedge_1m.tif")
scrub <- raster("E:/GIS_DATA/LandCover/UK/Devon_THaW/scrub_1m.tif")
## Get 100m cell numbers inside the ThaW region
xy <- xyFromCell(m100.t, 1:ncell(m100.t))
xyz <- na.omit(cbind(xy, raster::extract(m100.t, xy)))
cells <- cellFromXY(m100.t, xyz[,c("x","y")])
rm(xy)
head(cells)
cells[1:20]
which(cells==156482)/length(cells) ## Calculate how far through the list of cells we are
which(cells==156482)
.libPaths("C:\\SOFTWARE\\R-4.1.2\\library")
library(dplyr) ## bind_rows, sample_frac
library(MuMIn) # dredge() stdize() model.sel()
library(car) ## Variance Inflation Factor analysis
library(snow) ## for running on multiple cores of a cluster. Used for dredge function (best when used on a server rather than a desktop).
wd.out <- "E:/NON_PROJECT/DORMOUSE_NE/SDM/100m"
wd.env <- "E:/NON_PROJECT/DORMOUSE_NE/GIS/100m"
##### MAKE MULTIVARIATE MODELS #####
vars <- c('combo_broadl_m', 'combo_broadl_c', 'combo_conif_m', 'combo_conif_c', 'anc_wood', 'OS_Terrain_100_NS', 'OS_Terrain_100_WE',
'OS_Terrain_100_slope_pct', 'treehedge', 'scrub', "tasrng_win_5yrmn", "tasmax_spr_5yrmn", "tasmin_win_5yrmn", "sun_spr_5yrmn", "rainfall_spr_5yrmn")
dat <- read.csv(paste0(wd.env,"/dat_21Feb2022_t_sampled_prepped.csv"), as.is=T) ## Currently using NDMP and NDD+LRD samples, as only 49 NDMP presences
summary(dat)
table(dat$source)
wd.env
##### Scale explanatory variables as large numerical scope of variables can cause estimation problems (https://r-sig-mixed-models.r-project.narkive.com/fXcaHABA/r-sig-me-cholmod-warning-with-glmer). #####
## First save the values used to stdize, so as to graph and map later
# means <- apply(na.omit(dat[,vars]), 2, mean)
# sds <- apply(na.omit(dat[,vars]), 2, sd)
# destdize <- rbind(means, sds)
# write.csv(destdize, paste0(wd.out, "/dat_means_sds_22Feb2022_t_sampled.csv"), row.names=F)
destdize <- read.csv(paste0(wd.out, "/dat_means_sds_22Feb2022_t_sampled.csv"))
## Use method stdize from MuMin, which subtracts the mean and then divides by the standard deviation
dat.m <- na.omit(dat[,c("occ", vars, "source")]) ## NAs have to be removed for dredge function
dat.m <- dat.m[dat.m$source!="pseudo-absence",] ## remove points with non-NDMP presences
dat.m$source[dat.m$source %in% c("nonPTES-abs","PTES-abs")] <- "surv-abs"
dat.m[, vars] <- stdize(dat.m[,vars])
dat.m <- as.data.frame(cbind(dat.m, id=c(1:nrow(dat.m))))
summary(dat.m)
dim(dat.m)
table(dat.m$source)
## Use method stdize from MuMin, which subtracts the mean and then divides by the standard deviation
dat.m <- na.omit(dat[,c("occ", vars, "source")]) ## NAs have to be removed for dredge function
dat.m <- dat.m[dat.m$source!="pseudo-absence",] ## remove points with pseudo-absences
dat.m$source[dat.m$source %in% c("nonPTES-abs","PTES-abs")] <- "surv-abs"
dat.m[, vars] <- stdize(dat.m[,vars])
dat.m <- as.data.frame(cbind(dat.m, id=c(1:nrow(dat.m))))
dat.m <- dat.m[dat.m$source!="NDMP",] ## remove 49 points with NDMP presences
dim(dat.m)
write.csv(dat.m, paste0(wd.out, "/dat_std_22Feb2022_t_m_sampled_nddlrd.csv"), row.names=F)
### Calibration and validation data to make the model
dat.c <- sample_frac(cbind(dat.m), size=0.7)
write.csv(dat.c, paste0(wd.out, "/dat_std_22Feb2022_t_c_sampled_nddlrd.csv"), row.names=F)
dat.c[119,]
hist(dat.c$scrub)
which(dat.c[dat.c$scrub>10,])
dat.c$scrub>10
dat.c$scrub>8
table(dat$source)
hist(dat$scrub)
### Calibration and validation data to make the model
# dat.c <- sample_frac(cbind(dat.m), size=0.7)
# write.csv(dat.c, paste0(wd.out, "/dat_std_22Feb2022_t_c_sampled_nddlrd.csv"), row.names=F)
dat.c <- read.csv(paste0(wd.out, "/dat_std_22Feb2022_t_c_sampled_nddlrd.csv"))
dat.v <- dat.m[!(dat.m$id %in% dat.c$id),]
write.csv(dat.v, paste0(wd.out, "/dat_std_22Feb2022_t_v_sampled_nddlrd.csv"), row.names=F)
dat.v <- read.csv(paste0(wd.out, "/dat_std_22Feb2022_t_v_sampled_nddlrd.csv"))
##### Model with linear terms only #####
vars.m <- paste(vars, collapse=" + ")
ml <- glm(as.formula(paste0("occ ~ ", vars.m)), family=binomial, dat.c, na.action=na.fail) ## Make model with calibration data.
summary(ml)
## Check for variance inflation - an effect of collinear explanatory variables
vif(ml, singular.ok = TRUE) ## broadl and treehedge a bit high (~6)
## Dredge model with linear terms. Previous version only permitted one of the two highly correlated terms. Actually runs in a few mins on desktop.
# mld.all <- pdredge(ml, beta="sd", evaluate=T, trace=2, cluster=clust)
mld.all <- dredge(ml, beta="sd", evaluate=T, trace=2)
save(mld.all, file=paste0(wd.out, "/linear_dredged_all_t_sampled_nddlrd"))
# save(mld.all, file=paste0(wd.out, "/linear_dredged_all_t_sampled_nddlrd"))
# load(paste0(wd.out, "/linear_dredged_all_t_sampled_nddlrd"))
write.csv(mld.all, file=paste0(wd.out,"/linear_dredged_all_t_sampled_nddlrd.csv"), row.names=F)
# save(mld.all, file=paste0(wd.out, "/linear_dredged_all_t_sampled_nddlrd"))
# load(paste0(wd.out, "/linear_dredged_all_t_sampled_nddlrd"))
# write.csv(mld.all, file=paste0(wd.out,"/linear_dredged_all_t_sampled_nddlrd.csv"), row.names=F)
mld.all[mld.all$delta<=2,] ## 17 models fall into the best model subset. anc_wood, rainfall, scrub, sun, tasmax, tasmin, tasrange, and treehedge the consistent variables. NA, WE, and broadl and conif flip in and out.
# save(mld.all, file=paste0(wd.out, "/linear_dredged_all_t_sampled_nddlrd"))
# load(paste0(wd.out, "/linear_dredged_all_t_sampled_nddlrd"))
# write.csv(mld.all, file=paste0(wd.out,"/linear_dredged_all_t_sampled_nddlrd.csv"), row.names=F)
mld.all[mld.all$delta<=2,] ## 17 models fall into the best model subset. anc_wood, rainfall, scrub, sun, tasmax, tasmin, tasrange, and treehedge the consistent variables. NA, WE, and broadl and conif flip in and out.
# m.best <- get.models(md, 1)[[1]]
mld.all <- get.models(mld.all, subset=delta<2) ## mld is now an object containing the best models.
##### Model with linear and quadratic terms #####
### Get the linear terms that were selected in the top models and make into quadratic terms
vars.mld <- unique(unlist(lapply(mld.all, function(x) {rownames(summary(x)$coefficients)})))
vars.mld <- vars.mld[vars.mld!="(Intercept)"]
vars.mld
## RemoveDon't try quadratic effects of variables rarely included
vars.mld <- vars.mld[vars.mld!="scrub"] ## only in 2  model
vars.mld <- vars.mld[vars.mld!="combo_conif_m"] ## only in 2 model
vars.mq <- paste0("I(", vars.mld, "^2)") ## quadratic terms
vars.mlq <- paste(c(vars, vars.mq), collapse=" + ") ## all linear and quadratic terms. was vars.mld
### Dredge the model with all linear terms retained in the best model subset, and all of their quadratic terms
mlq <- glm(as.formula(paste0("occ ~ ", vars.mlq)), family=binomial, dat.c, na.action=na.fail) ## Make model with calibration data. Fitted probabilities 0 or 1 occur if line 119 is not removed
vif(mlq) ## combo_broadl_m (9.7) borderline. conif_m and anc_wood super high, but seems to be correlating strongly with quadratic forms
vars.mq
vars.mlq1 <- paste(c(vars, vars.mq[1:5]), collapse=" + ") ## all linear and first five quadratic terms
mlq1 <- glm(as.formula(paste0("occ ~ ", vars.mlq1)), family=binomial, dat.c, na.action=na.fail)
vars.mlq2 <- paste(c(vars, vars.mq[6:11]), collapse=" + ") ## all linear and first five quadratic terms
mlq2 <- glm(as.formula(paste0("occ ~ ", vars.mlq2)), family=binomial, dat.c, na.action=na.fail)
vars.mq
## Retain quadratic term only if linear term is added
msubset <- expression(dc("anc_wood", "I(anc_wood^2)") & ##
dc("rainfall_spr_5yrmn", "I(rainfall_spr_5yrmn^2)") & ##
dc("sun_spr_5yrmn", "I(sun_spr_5yrmn^2)") & ##
dc("tasmax_spr_5yrmn", "I(tasmax_spr_5yrmn^2)") & ##
dc("tasmin_win_5yrmn", "I(tasmin_win_5yrmn^2)") & ##
dc("tasrng_win_5yrmn", "I(tasrng_win_5yrmn^2)") & ##
dc("treehedge", "I(treehedge^2)") & ##
dc("combo_conif_c", "I(combo_conif_c^2)") &
dc("OS_Terrain_100_NS", "I(OS_Terrain_100_NS^2)") &
dc("OS_Terrain_100_slope_pct", "I(OS_Terrain_100_slope_pct^2)") &
dc("OS_Terrain_100_WE", "I(OS_Terrain_100_WE^2)"))
# mlqd <- pdredge(mlq, beta="sd", evaluate=T, trace=2, subset=msubset, cluster=clust)
mlqd1 <- dredge(mlq1, beta="sd", evaluate=T, trace=2, subset=msubset) ## lengthy
warnings()
save(mlqd1, file=paste0(wd.out, "/linear_quadratic_dredged_t_sampled1_nddlrd"))
write.csv(mlqd1, file=paste0(wd.out,"/linear_quadratic_dredged_t_sampled1_nddlrd.csv"), row.names=T)
mlqd1[mlqd1$delta<=2,] ## 23 models in the best model subset.
mlqd2 <- dredge(mlq2, beta="sd", evaluate=T, trace=2, subset=msubset) ## lengthy
save(mlqd2, file=paste0(wd.out, "/linear_quadratic_dredged_t_sampled2_nddlrd"))
write.csv(mlqd2, file=paste0(wd.out,"/linear_quadratic_dredged_t_sampled2_nddlrdwarn.csv"), row.names=F)
mlqd2[mlqd2$delta<=2,] ## Some overlap in AIC between mlqd1 and 2.
summary(dat.m)
summary(dat.c)
table(dat.c$source)
summary(dat.c)
summary(mlqd1)
mlqd1[mlqd1$delta<=2,] ## 14 models in the best model subset. Ancient woodland now positive but humped
####### CORRELATIONS #####
## df is a data frame in which the first column is presence/absence (or abundance not including 0s) and the rest are the environmental variables.
pa <- c(rep(0,25), rep1,25))
pa
####### CORRELATIONS #####
## df is a data frame in which the first column is presence/absence (or abundance not including 0s) and the rest are the environmental variables.
pa <- c(rep(0,25), rep(1,25))
corrs <- cor(df[-1], method = c("spearman"))
pa
env <- as.matrix(runif(200,0,100), nrow=50)
summary(env)
env <- as.matrix(runif(200,0,100), nrow=50, ncol=4)
summary(env)
env <- matrix(runif(200,0,100), nrow=50, ncol=4)
env
df <- as.data.frame(cbind(pa, env))
summary(df)
colnames(df <- c("pa","env1","env2","env3","env4"))
df <- as.data.frame(cbind(pa, env))
colnames(df) <- c("pa","env1","env2","env3","env4"))
summary(df)
colnames(df) <- c("pa","env1","env2","env3","env4")
summary(df)
####### CORRELATIONS #####
corrs <- cor(df[-1], method = c("spearman"))
corrs
corrplot(corrs)
library(corrplot)
####### CORRELATIONS #####
corrs <- cor(df[-1], method = c("spearman"))
corrplot(corrs)
#### Extract correlations > 0.7 and tabulate
corrs2 <- rcorr(as.matrix(df[,-1]))
corrs2$r[lower.tri(corrs2$r)] <- NA
a <- as.data.frame(which(abs(corrs2$r) >= 0.7 & abs(corrs2$r) != 1, arr.ind = TRUE))
##  correlation >= 0.7
r <- rownames(corrs2$r)[a$row]
c <- colnames(corrs2$r)[a$col]
corrs2
#### Extract correlations > 0.7 and tabulate
corrs2 <- rcorr(as.matrix(df[,-1]))
library(Hmisc) ## rcorr
#### Extract correlations > 0.7 and tabulate
corrs2 <- rcorr(as.matrix(df[,-1]))
corrs2$r[lower.tri(corrs2$r)] <- NA
a <- as.data.frame(which(abs(corrs2$r) >= 0.7 & abs(corrs2$r) != 1, arr.ind = TRUE))
##  correlation >= 0.7
r <- rownames(corrs2$r)[a$row]
c <- colnames(corrs2$r)[a$col]
corr.out <- data.frame(matrix(nrow=nrow(a), ncol=2, dimnames=list(c(), c("vars", "corrs"))), stringsAsFactors=F)
corr.out$vars <- paste0(r, " & ", c)
corr.out$corrs <- apply(a, MARGIN=1, FUN=function(x) { corrs2$r[x[1], x[2]] }) ##
corr.out
corrs2
r
a
##### DECIDE WHICH CORRELATED VARIABLES TO RETAIN #####
### Calibration and validation data
df$id <- 1:nrow(df)
dat.c <- sample_frac(df, size=0.7)
dat.v <- df[!(df$id %in% dat.c$id),]
dat.c <- sample_frac(df, size=0.7)
library(dplyr) ## sample_frac
dat.c <- sample_frac(df, size=0.7)
dat.v <- df[!(df$id %in% dat.c$id),]
summary(dat.c)
summary(dat.v)
dim(dat.c)
dim(dat.v)
### Variables to choose between - for variables that are measured in teh same seasons.
vars <- paste0(c, c("env1", "env2", "env3"))
v <- vars[1]
m <- glm(as.formula(paste0("occ ~ ", v)), family=binomial, dat.c) ## Make model with calibration data
m <- glm(as.formula(paste0("pa ~ ", v)), family=binomial, dat.c) ## Make model with calibration data
p <- predict(m, dat.v, re.form=NA, type="response") ## Predict model with validation data
roc_obj <- roc(dat.v$occ, p)
print(paste(v, round(auc(roc_obj),3)))
library(pROC) ## AUC
roc_obj <- roc(dat.v$occ, p)
roc_obj <- roc(dat.v$occ, p)
roc_obj <- roc(dat.v$pa, p)
print(paste(v, round(auc(roc_obj),3)))
### Variables to choose between - for vasriables that are measured in teh same seasons.
for (c in c("tasmax", "sun")) {
vars <- paste0(c, c("_spr_5yrmn", "_smr_5yrmn", "_aut_5yrmn"))
# vars <- c("rainfall_spr_5yrmn", "rainfall_smr_5yrmn", "rainfall_aut_5yrmn")
for (v in vars) {
m <- glm(as.formula(paste0("occ ~ ", v)), family=binomial, dat.c) ## Make model with calibration data
p <- predict(m, dat.v, re.form=NA, type="response") ## Predict model with validation data
roc_obj <- roc(dat.v$occ, p)
print(paste(v, round(auc(roc_obj),3)))
}
}
vars <- paste0(c, c("env1", "env2", "env3"))
for (v in vars) {
m <- glm(as.formula(paste0("pa ~ ", v)), family=binomial, dat.c) ## Make model with calibration data
p <- predict(m, dat.v, re.form=NA, type="response") ## Predict model with validation data
roc_obj <- roc(dat.v$pa, p)
print(paste(v, round(auc(roc_obj),3))) ## May well want to use other measures of goodness of fit (e.g. deviance explained) or predictive poser (e.g. TSS)
}
}
vars <- paste0(c, c("env1", "env2", "env3"))
for (v in vars) {
m <- glm(as.formula(paste0("pa ~ ", v)), family=binomial, dat.c) ## Make model with calibration data
p <- predict(m, dat.v, re.form=NA, type="response") ## Predict model with validation data
roc_obj <- roc(dat.v$pa, p)
print(paste(v, round(auc(roc_obj),3))) ## May well want to use other measures of goodness of fit (e.g. deviance explained) or predictive poser (e.g. TSS)
}
### Variables to choose between - for variables that are measured in teh same seasons.
vars <- paste0(c("env1", "env2", "env3"))
for (v in vars) {
m <- glm(as.formula(paste0("pa ~ ", v)), family=binomial, dat.c) ## Make model with calibration data
p <- predict(m, dat.v, re.form=NA, type="response") ## Predict model with validation data
roc_obj <- roc(dat.v$pa, p)
print(paste(v, round(auc(roc_obj),3))) ## May well want to use other measures of goodness of fit (e.g. deviance explained) or predictive poser (e.g. TSS)
}
m <- glm(as.formula(paste0("pa ~ ", poly(v,2))), family=binomial, dat.c) ## Make model with calibration data. May well want to use another method as well, e.g. random forest
poly(v,2)
paste0("pa ~ poly(",v,",2)")
s.formula(paste0("pa ~ poly(",v,",2)"))
as.formula(paste0("pa ~ poly(",v,",2)"))
m <- glm(as.formula(paste0("pa ~ poly(",v,",2)")), family=binomial, dat.c) ## Make model with calibration data. May well want to use another method as well, e.g. random forest
p <- predict(m, dat.v, re.form=NA, type="response") ## Predict model with validation data
roc_obj <- roc(dat.v$pa, p)
print(paste(v, round(auc(roc_obj),3))) ## May well want to use other measures of goodness of fit (e.g. deviance explained) or predictive poser (e.g. TSS)
summary(m)
corrs
####### CORRELATIONS #####
corrs <- cor(df[,-1], method = c("spearman")) ## Save with seperate name for each species. Probably not always necessary. Just instructive to look at to begin with.
corrs
df[-1]
df[-6]
env.nms <- c("env1","env2","env3","env4")
##### DATA #####
## df is a data frame in which the first column is presence/absence (or abundance not including 0s) and the rest are the environmental variables.
pa <- c(rep(0,25), rep(1,25))
env <- matrix(runif(200,0,100), nrow=50, ncol=4)
df <- as.data.frame(cbind(pa, env))
env.nms <- c("env1","env2","env3","env4")
colnames(df) <- c("pa",env.nms)
vars <- c("env1","env2","env3","env4")
colnames(df) <- c("pa",vars)
## Standardise environmental variables
df[vars]  <- stdize(df[,vars])
## Standardise environmental variables
df[,vars]  <- stdize(df[,vars])
